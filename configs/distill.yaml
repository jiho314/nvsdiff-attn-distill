# Distill 
distill_config:
  distill_pairs:  [
    [1, "track_head"],
    [5, "track_head"],
    [9, "track_head"],
    [13, "track_head"],
    [17, "track_head"],
  ] # student(flux), teacher(vggt)(0~23, "track_head", "point_head")
  distill_loss_weight: 0.05 # 0.05 per layer 
  distill_loss_fn: "cross_entropy"  # "cross_entropy", "l1", ...
  vggt_logit_head: "softmax_headmean"  # "softmax_headmean", "identity", "softmax_headmlp"
  vggt_logit_head_kwargs: {"softmax_temp": 8.0}
  unet_logit_head: "softmax_headmean"  # "softmax_headmean", "identity", "softmax_headmlp"
  unet_logit_head_kwargs: {"softmax_temp": 1.0} 

  distill_query: "target" # "reference", "target", "all"
  distill_key: "reference" # "reference", "target", "self" "all"



# feasibility
# batch=8, tgt/ref=1/3, (2, track_head) : 42GB

